{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export OMP_NUM_THREADS=32\n",
    "\n",
    "import os \n",
    "import omegaconf\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "path_dir_notebook_file = \"\"                                         # IMPORTANT: Define the directory's path \n",
    "                                                                    # where the notebook file is contained.\n",
    "assert os.path.isdir(path_dir_notebook_file) == True, \"Notebook's directory {} does not exist!\".format(path_dir_notebook_file)\n",
    "os.chdir(path_dir_notebook_file)\n",
    "\n",
    "path_dir_configs = os.path.join(path_dir_notebook_file, \"configs\")\n",
    "\n",
    "filename_config_file = \"all_NO2_Italian_stations_2020_2020.yaml\"    # IMPORTANT: Define the configuration file that has to be performed.\n",
    "path_config_file = os.path.join(path_dir_configs, filename_config_file)\n",
    "assert os.path.isfile(path_config_file) == True, \"Config file {} does not exist!\".format(path_config_file)\n",
    "\n",
    "cfg = omegaconf.OmegaConf.load(path_config_file)\n",
    "\n",
    "# ------------------ Libraries ------------------\n",
    "\n",
    "# Set default GPU to use\n",
    "gpu_default = cfg['gpu_default']\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import pyro\n",
    "from pyro.infer.autoguide import AutoDelta, AutoDiagonalNormal, AutoLowRankMultivariateNormal\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Callable\n",
    "import graphviz\n",
    "import pyro.poutine as poutine\n",
    "import random\n",
    "import json\n",
    "from math import ceil\n",
    "\n",
    "from pyro.infer import SVI, Trace_ELBO, Predictive, TracePredictive, TraceMeanField_ELBO\n",
    "from tqdm.auto import trange, tqdm\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import gc\n",
    "\n",
    "# Make PyroModule parameters local (like ordinary torch.nn.Parameters),\n",
    "# rather than shared by name through Pyro's global parameter store.\n",
    "# This is highly recommended whenever models can be written without pyro.param().\n",
    "pyro.settings.set(module_local_params=True)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = cfg['seed']\n",
    "pyro.set_rng_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(\"Cuda is available: {}\".format(USE_CUDA))\n",
    "\n",
    "if USE_CUDA:\n",
    "  torch.cuda.set_device(gpu_default)\n",
    "  print(\"Default GPU CUDA: {}\".format(torch.cuda.current_device()))\n",
    "  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from dataset import *\n",
    "from BYADNN import *\n",
    "from utils import *\n",
    "from utils_pyro import *\n",
    "\n",
    "# ------------------ Dataset information ------------------\n",
    "local_dir_datasets = os.path.join(path_dir_notebook_file, cfg['dataset_info']['path_dir_datasets'])     # Define the complete path of directory where are contained all datasets\n",
    "assert os.path.isdir(local_dir_datasets) == True, \"Datasets's directory {} does not exist!\".format(local_dir_datasets)\n",
    "\n",
    "gp_method = cfg['dataset_info']['gp_method']\n",
    "air_poll_selected = cfg['dataset_info']['air_poll_selected']\n",
    "freq_mode = cfg['dataset_info']['freq_mode']\n",
    "start_year = cfg['dataset_info']['start_year']\n",
    "end_year = cfg['dataset_info']['end_year']\n",
    "type_station = cfg['dataset_info']['type_station']\n",
    "n_stations = cfg['dataset_info']['n_stations']\n",
    "co_in_ug_m3 = cfg['dataset_info']['co_in_ug_m3']\n",
    "round_prediction = cfg['dataset_info']['round_prediction']\n",
    "remove_imp_target_training = cfg['dataset_info']['remove_imp_target_training']\n",
    "org_data = cfg['dataset_info']['org_data']\n",
    "standardization_data = cfg['dataset_info']['standardization_data']\n",
    "\n",
    "dict_limit_air_pollutants = cfg['dict_limit_air_pollutants']\n",
    "\n",
    "dataset_name = \"org_{}_{}_{}_{}_{}_cod_stations_1x\".format(air_poll_selected, start_year, end_year, type_station, n_stations)\n",
    "\n",
    "# ------------------ Model configuration ------------------\n",
    "dict_model_config = cfg['model']\n",
    "model_name = cfg['defaults']['model']\n",
    "\n",
    "# ------------------ Experiment configuration ------------------\n",
    "dict_exp_config = cfg['experiment']\n",
    "batch_size = cfg['experiment']['batch_size']\n",
    "\n",
    "# ------------------ Experiment's status ------------------\n",
    "resume_training = False\n",
    "eval_model = False\n",
    "dataloader_saved = False\n",
    "plot_histo_dataset = False\n",
    "\n",
    "start_date = datetime(start_year, 1, 1, 0, 0)\n",
    "end_date = datetime(end_year+1, 1, 1, 0, 0)\n",
    "\n",
    "if freq_mode == \"hour\":\n",
    "  delta_time = timedelta(hours=1)\n",
    "else:\n",
    "  delta_time = timedelta(days=1)\n",
    "\n",
    "if gp_method:\n",
    "  path_dir_dataset = joinpath(local_dir_datasets, \"GP\")\n",
    "else:\n",
    "  path_dir_dataset = joinpath(local_dir_datasets, \"D2_K2\")\n",
    "\n",
    "path_dir_dataset = joinpath(path_dir_dataset, dataset_name)\n",
    "path_txt_cod_stations = joinpath(path_dir_dataset, \"cod_stations.txt\")\n",
    "path_dir_dataset = joinpath(path_dir_dataset, round_prediction)\n",
    "\n",
    "path_idx_missing_values_train = joinpath(path_dir_dataset, \"all_idx_missing_values_train.pickle\")\n",
    "path_idx_missing_values_complete = joinpath(path_dir_dataset, \"all_complete_idx_missing_values.pickle\")\n",
    "path_idx_train = joinpath(path_dir_dataset, \"all_idx_train.pickle\")\n",
    "path_idx_test = joinpath(path_dir_dataset, \"all_idx_test.pickle\")\n",
    "path_x_train = joinpath(path_dir_dataset, \"all_x_train.pickle\")\n",
    "path_y_train = joinpath(path_dir_dataset, \"all_y_train.pickle\")\n",
    "path_x_test = joinpath(path_dir_dataset, \"all_x_test.pickle\")\n",
    "path_y_test = joinpath(path_dir_dataset, \"all_y_test.pickle\")\n",
    "path_indo_cod_station = joinpath(path_dir_dataset, \"info_cod_stations.pickle\")\n",
    "\n",
    "list_cod_stations = []\n",
    "\n",
    "if os.path.exists(path_txt_cod_stations):\n",
    "  with open(path_txt_cod_stations) as f:\n",
    "      list_cod_stations = [line.rstrip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionaries of all cod stations selected\n",
    "info_cod_stations, all_idx_missing_values_train, all_idx_missing_values_complete, \\\n",
    "all_idx_train, all_idx_test, all_x_train, all_y_train, \\\n",
    "all_x_test, all_y_test = load_dictionaries_dataset(     path_indo_cod_station, path_idx_missing_values_train, \\\n",
    "                                                        path_idx_missing_values_complete, path_idx_train, path_idx_test, \\\n",
    "                                                        path_x_train, path_y_train, path_x_test, path_y_test \\\n",
    "                                                )\n",
    "\n",
    "dict_list_dates = compute_dates(start_year, end_year, freq_mode)   \n",
    "\n",
    "max_model_order = get_maximum_model_order(info_cod_stations, all_x_train)\n",
    "print(\"Max model order: {}\".format(max_model_order))\n",
    "\n",
    "new_x_train_padding, new_x_test_padding, \\\n",
    "dict_dates_train, dict_dates_test =  dataset_padding(   info_cod_stations, all_x_train, all_x_test,\n",
    "                                                        dict_list_dates, all_idx_train, all_idx_test, max_model_order   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Dataset loading #######################\n",
    "path_dir_model_config = joinpath(path_dir_dataset, cfg['experiment']['name_exp'])\n",
    "os.makedirs(path_dir_model_config, exist_ok=True)\n",
    "\n",
    "path_dataloaders_dir = joinpath(path_dir_model_config, \"dataloaders\")\n",
    "os.makedirs(path_dataloaders_dir, exist_ok=True)\n",
    "\n",
    "path_dataloader_train = joinpath(path_dataloaders_dir, \"train_loader.pth\")\n",
    "path_dict_datasets_train = joinpath(path_dataloaders_dir, \"dict_datasets_train.pkl\")\n",
    "path_dict_datasets_test = joinpath(path_dataloaders_dir, \"dict_datasets_test.pkl\")\n",
    "\n",
    "path_csv_errors_cod_stations = joinpath(path_dir_model_config, \"complete_errors.csv\")\n",
    "path_csv_calibration_errors_cod_stations_train = joinpath(path_dir_model_config, \"calibration_errors_train.csv\")\n",
    "path_csv_calibration_errors_cod_stations_test = joinpath(path_dir_model_config, \"calibration_errors_test.csv\")\n",
    "path_csv_ence_errors_cod_stations_train = joinpath(path_dir_model_config, \"ence_errors_train.csv\")\n",
    "path_csv_ence_errors_cod_stations_test = joinpath(path_dir_model_config, \"ence_errors_test.csv\")\n",
    "\n",
    "if os.path.exists(path_csv_calibration_errors_cod_stations_train):\n",
    "   os.remove(path_csv_calibration_errors_cod_stations_train)\n",
    "\n",
    "if os.path.exists(path_csv_calibration_errors_cod_stations_test):\n",
    "   os.remove(path_csv_calibration_errors_cod_stations_test)\n",
    "\n",
    "if os.path.exists(path_csv_ence_errors_cod_stations_train):\n",
    "   os.remove(path_csv_ence_errors_cod_stations_train)\n",
    "\n",
    "if os.path.exists(path_csv_ence_errors_cod_stations_test):\n",
    "   os.remove(path_csv_ence_errors_cod_stations_test)\n",
    "\n",
    "path_dir_weights = joinpath(path_dir_model_config, \"weights\")\n",
    "os.makedirs(path_dir_weights, exist_ok=True)\n",
    "\n",
    "path_dir_plots = joinpath(path_dir_model_config, \"plots\")\n",
    "os.makedirs(path_dir_plots, exist_ok=True)\n",
    "\n",
    "path_dir_models = joinpath(path_dir_model_config, \"models\")\n",
    "os.makedirs(path_dir_models, exist_ok=True)\n",
    "\n",
    "path_log_txt = joinpath(path_dir_model_config, \"log.txt\")\n",
    "\n",
    "path_cod_stations_processed = joinpath(path_dir_model_config, \"cod_stations_processed.txt\")\n",
    "path_csv_ts_cod_stations = joinpath(path_dir_model_config, \"ts_cod_stations_imputed.csv\")\n",
    "\n",
    "all_new_x_train = {}\n",
    "all_new_y_train = {}\n",
    "\n",
    "list_cod_stations_processed = []\n",
    "\n",
    "if os.path.exists(path_cod_stations_processed):\n",
    "  with open(path_cod_stations_processed, 'r') as f:\n",
    "    list_cod_stations_processed = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "dict_datasets_train = {}\n",
    "dict_datasets_test = {}\n",
    "list_len_datasets = []\n",
    "\n",
    "if resume_training == True or eval_model == True or dataloader_saved == True:\n",
    "  train_loader = torch.load(path_dataloader_train)\n",
    "\n",
    "  with open(path_dict_datasets_train, 'rb') as fp:\n",
    "      dict_datasets_train = pickle.load(fp)\n",
    "\n",
    "  with open(path_dict_datasets_test, 'rb') as fp:\n",
    "      dict_datasets_test = pickle.load(fp)\n",
    "else:\n",
    "  # Load cod station\n",
    "  for key in info_cod_stations.keys():\n",
    "    \n",
    "    # Clear param store\n",
    "    pyro.clear_param_store()\n",
    "      \n",
    "    current_cod_station = info_cod_stations[key][0]\n",
    "\n",
    "    if current_cod_station not in list_cod_stations_processed or eval_model == True:\n",
    "      print(\"Current cod station: {}\".format(current_cod_station))\n",
    "\n",
    "      model_order = info_cod_stations[key][1]\n",
    "      train_dataset = CustomDataSet_Train(current_cod_station, key, info_cod_stations, batch_size, new_x_train_padding, all_y_train, all_idx_missing_values_train, \n",
    "                                          model_order, max_model_order, dict_dates_train[key], remove_imp_target_training=remove_imp_target_training,\n",
    "                                          org_data=org_data, standardization_data=standardization_data\n",
    "      )\n",
    "\n",
    "      mean_cod_station = train_dataset.mean\n",
    "      std_cod_station = train_dataset.std\n",
    "\n",
    "      x_test = new_x_test_padding[key]\n",
    "      y_test = all_y_test[key]\n",
    "\n",
    "      test_dataset  = CustomDataSet_Test(current_cod_station, key, info_cod_stations, x_test, y_test, model_order, max_model_order, dict_dates_test[key],\n",
    "                                        org_data=org_data, standardization_data=standardization_data,\n",
    "                                        mean=mean_cod_station, std=std_cod_station)\n",
    "\n",
    "      # Histrogram plot of i-th cod station time series\n",
    "      if plot_histo_dataset:\n",
    "        path_dir_plots_histo = joinpath(path_dir_plots, current_cod_station)\n",
    "        path_dir_plots_histo = joinpath(path_dir_plots_histo, \"histo\")\n",
    "        os.makedirs(path_dir_plots_histo, exist_ok=True)\n",
    "\n",
    "        compute_and_plot_histogram(all_y_train[key], dict_exp_config['n_bins'], train_dataset.min_cod_station, train_dataset.max_cod_station, train_dataset.mean, train_dataset.std, org_data, standardization_data, current_cod_station, True, path_dir_plots_histo)\n",
    "        compute_and_plot_histogram(all_y_test[key], dict_exp_config['n_bins'], test_dataset.min_cod_station, test_dataset.max_cod_station, test_dataset.mean, test_dataset.std, org_data, standardization_data, current_cod_station, False, path_dir_plots_histo)\n",
    "\n",
    "      if batch_size > len(train_dataset):\n",
    "        batch_size = len(train_dataset)\n",
    "      \n",
    "      list_len_datasets.append(len(train_dataset))\n",
    "\n",
    "      dict_datasets_train[key] = train_dataset\n",
    "      dict_datasets_test[key] = test_dataset\n",
    "\n",
    "  final_dataset = torch.utils.data.ConcatDataset(dict_datasets_train.values())\n",
    "\n",
    "  list_indices = []\n",
    "  last_idx = 0\n",
    "  for i in range(len(list_len_datasets)):\n",
    "    \n",
    "    len_dataset_batch_size = 0\n",
    "    n_batches_current_dataset = int(list_len_datasets[i] / batch_size)\n",
    "\n",
    "    if n_batches_current_dataset == 0:\n",
    "      print(\"ERROR: The dimension of this dataset is less than the batch size specified\")\n",
    "      exit(-1)\n",
    "\n",
    "    len_dataset_batch_size = n_batches_current_dataset * batch_size\n",
    "    \n",
    "    indices = list(range(last_idx, last_idx + len_dataset_batch_size))\n",
    "    last_idx += len_dataset_batch_size\n",
    "    list_indices.append(indices)\n",
    "\n",
    "  batch_sampler = MyBatchSampler(list_indices, batch_size)\n",
    "  train_loader = DataLoader(final_dataset, batch_sampler=batch_sampler, pin_memory=True)\n",
    "\n",
    "  torch.save(train_loader, path_dataloader_train)\n",
    "\n",
    "  with open(path_dict_datasets_train, \"wb\") as fp:\n",
    "      pickle.dump(dict_datasets_train, fp)\n",
    "\n",
    "  with open(path_dict_datasets_test, \"wb\") as fp:\n",
    "      pickle.dump(dict_datasets_test, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Training and Inference module #####################\n",
    "\n",
    "if resume_training == False:\n",
    "  file_log = open(path_log_txt, \"w\")\n",
    "  file_log.write(\"Len training set: {}\\n\".format(len(train_loader.dataset)))\n",
    "  file_log.write(\"Input features: {}\\n\".format(dict_model_config[\"input_features\"]))\n",
    "  file_log.write(\"Max model order: {}\\n\".format(max_model_order))\n",
    "  file_log.flush()\n",
    "else:\n",
    "  file_log = open(path_log_txt, \"a\")\n",
    "\n",
    "path_log_txt = joinpath(path_dir_model_config, \"log.txt\")\n",
    "\n",
    "# Create a model and a guide, both as (Pyro)Modules.\n",
    "model: torch.nn.Module = Model( len_dataset=len(train_loader.dataset), input_features=dict_model_config[\"input_features\"], seq_len=max_model_order, \n",
    "                                embedding_dim=dict_model_config[\"embedding_dim\"], num_enc_block=dict_model_config[\"num_enc_block\"], num_heads=dict_model_config[\"num_heads\"], \n",
    "                                h_enc_layer=dict_model_config[\"h_enc_layer\"], dropout=dict_model_config[\"dropout\"], h1=dict_model_config[\"h1\"], h2=dict_model_config[\"h2\"], \n",
    "                                add_positional_encoding=dict_model_config[\"add_positional_encoding\"],\n",
    "                                add_temporal_embedding=dict_model_config[\"add_temporal_embedding\"],\n",
    "                                sigma_layer=dict_model_config[\"sigma_layer\"], device=device).to(device)\n",
    "model.train()\n",
    "\n",
    "# Variational Posterior\n",
    "if dict_exp_config[\"guide\"] == \"AutoDelta\":\n",
    "  guide: torch.nn.Module = AutoDelta(model)\n",
    "elif dict_exp_config[\"guide\"] == \"AutoDiagonalNormal\":\n",
    "  guide: torch.nn.Module = AutoDiagonalNormal(model)\n",
    "elif dict_exp_config[\"guide\"] == \"AutoLowRankMultivariateNormal\":\n",
    "  guide: torch.nn.Module = AutoLowRankMultivariateNormal(model)\n",
    "else:\n",
    "  pass\n",
    "\n",
    "path_loss_train_txt = joinpath(path_dir_models, \"loss_train.txt\")\n",
    "path_loss_test_txt = joinpath(path_dir_models, \"loss_test.txt\")\n",
    "path_best_model_txt = joinpath(path_dir_models, \"best_model.txt\")\n",
    "\n",
    "if resume_training == False:\n",
    "  path_architecture_txt = joinpath(path_dir_models, \"architecture.txt\")\n",
    "  path_config_model_txt = joinpath(path_dir_models, \"config_model.txt\")\n",
    "  path_config_exp_txt = joinpath(path_dir_models, \"config_exp.txt\")\n",
    "\n",
    "  file_loss_train = open(path_loss_train_txt, \"w\")\n",
    "  file_loss_test = open(path_loss_test_txt, \"w\")\n",
    "\n",
    "  with open(path_architecture_txt, 'w') as f:\n",
    "    f.write(str(model) + \"\\n\")\n",
    "\n",
    "  with open(path_config_model_txt, 'w') as f:\n",
    "    f.write(json.dumps(OmegaConf.to_container(dict_model_config)))\n",
    "\n",
    "  with open(path_config_exp_txt, 'w') as f:\n",
    "    f.write(json.dumps(OmegaConf.to_container(dict_exp_config)))\n",
    "  \n",
    "# Create a loss function as a Module that includes model and guide parameters.\n",
    "# All Pyro ELBO estimators can be __call__()ed with a model and guide pair as arguments\n",
    "# to return a loss function Module that takes the same arguments as the model and guide\n",
    "# and exposes all of their torch.nn.Parameters and pyro.nn.PyroParam parameters.\n",
    "elbo: Callable[[torch.nn.Module, torch.nn.Module], torch.nn.Module] = Trace_ELBO(vectorize_particles=True)\n",
    "loss: torch.nn.Module = elbo(model, guide)\n",
    "loss.to(device=torch.device(device))\n",
    "\n",
    "# All relevant parameters need to be initialized before an optimizer can be created.\n",
    "# Since we used AutoNormal guide our parameters have not be initialized yet.\n",
    "# Therefore we initialize the model and guide by running one mini-batch through the loss.\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "x_batch = batch['x']\n",
    "y_batch = batch['y']\n",
    "mask = batch['mask'][0]  \n",
    "date_batch = batch['date']\n",
    "\n",
    "if USE_CUDA:\n",
    "  x_batch = x_batch.cuda()\n",
    "  y_batch = y_batch.cuda()\n",
    "  mask = mask.cuda()\n",
    "  date_batch = [date_batch[0].cuda(), date_batch[1].cuda(), date_batch[2].cuda(), date_batch[3].cuda(), date_batch[4].cuda()]\n",
    "\n",
    "loss(x_batch, date_batch, mask, y_batch)\n",
    "\n",
    "# Create a PyTorch optimizer for the parameters of the model and guide in loss_fn.\n",
    "if dict_exp_config[\"optimizer\"] == \"Adam\":\n",
    "  optimizer = torch.optim.Adam(loss.parameters(), lr=dict_exp_config[\"lr\"], weight_decay=dict_exp_config[\"weight_decay\"], \n",
    "                               betas=dict_exp_config[\"betas\"])\n",
    "\n",
    "# --------------------- Training phase ---------------------\n",
    "best_mse_test = np.inf\n",
    "best_mse_test_org = np.inf\n",
    "best_elbo_test = np.inf\n",
    "best_epoch = -1\n",
    "best_model_state_dict = None\n",
    "path_last_model = joinpath(path_dir_weights, \"last_model.pt\")\n",
    "path_last_model_params = joinpath(path_dir_weights, \"last_model_params.pt\")\n",
    "path_best_model = joinpath(path_dir_weights, \"best_model.pt\")\n",
    "path_best_model_params = joinpath(path_dir_weights, \"best_model_params.pt\")\n",
    "\n",
    "tot_train_elbo = []\n",
    "test_elbo = []\n",
    "\n",
    "if resume_training:\n",
    "  file_loss_train = open(path_loss_train_txt, \"r\")\n",
    "  file_loss_test = open(path_loss_test_txt, \"r\")\n",
    "  file_best_model = open(path_best_model_txt, \"r\")\n",
    "\n",
    "  lines_file_loss_train = file_loss_train.readlines()\n",
    "  lines_file_loss_test = file_loss_test.readlines()\n",
    "  lines_file_best_model = file_best_model.readlines()\n",
    "\n",
    "  for line in lines_file_loss_train:\n",
    "    tot_train_elbo.append(float(line))\n",
    "  \n",
    "  for line in lines_file_loss_test:\n",
    "    test_elbo.append(float(line))\n",
    "  \n",
    "  file_loss_train.close()\n",
    "  file_loss_test.close()\n",
    "\n",
    "  file_loss_train = open(path_loss_train_txt, \"a\")\n",
    "  file_loss_test = open(path_loss_test_txt, \"a\")\n",
    "\n",
    "  best_epoch = int(lines_file_best_model[0])\n",
    "  best_mse_test = float(lines_file_best_model[1])\n",
    "  if org_data == False:\n",
    "    best_mse_test_org = float(lines_file_best_model[2])\n",
    "    best_elbo_test = float(lines_file_best_model[3])\n",
    "  else:\n",
    "    best_elbo_test = float(lines_file_best_model[2])\n",
    "\n",
    "# Clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "if resume_training:\n",
    "  model_state_dict, guide, optimizer_state_dict, scheduler_state_dict, epoch = load_checkpoint(path_last_model)\n",
    "elif eval_model:\n",
    "  model_state_dict, guide, optimizer_state_dict, scheduler_state_dict, best_epoch = load_checkpoint(path_best_model)\n",
    "\n",
    "last_epoch_scheduler = -1\n",
    "if resume_training:\n",
    "  last_epoch_scheduler = epoch\n",
    "\n",
    "if resume_training == False:\n",
    "  if dict_exp_config[\"lr_scheduler\"] is not None:\n",
    "    if dict_exp_config[\"lr_scheduler\"] == \"OneCycleLR\":\n",
    "      scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=dict_exp_config[\"lr\"], steps_per_epoch=dict_exp_config[\"steps_per_epoch\"], epochs=dict_exp_config[\"num_epochs\"], last_epoch=last_epoch_scheduler)\n",
    "    elif dict_exp_config[\"lr_scheduler\"] == \"MultiStepLR\":\n",
    "      scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=dict_exp_config[\"milestones\"], gamma=0.1, last_epoch=last_epoch_scheduler)\n",
    "    elif dict_exp_config[\"lr_scheduler\"] == \"CosineAnnealingLR\":\n",
    "      scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=dict_exp_config[\"steps_per_epoch\"], eta_min=0, last_epoch=last_epoch_scheduler)\n",
    "    elif dict_exp_config[\"lr_scheduler\"] == \"CosineWarmupScheduler\":\n",
    "      scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=dict_exp_config[\"warmup\"], max_iters=dict_exp_config[\"num_epochs\"], last_epoch=last_epoch_scheduler)\n",
    "  else:\n",
    "    scheduler = None\n",
    "\n",
    "if resume_training or eval_model:\n",
    "\n",
    "  model.load_state_dict(model_state_dict)\n",
    "  optimizer.load_state_dict(optimizer_state_dict)\n",
    "\n",
    "  loss: torch.nn.Module = elbo(model, guide)\n",
    "  loss.to(device=torch.device(device))\n",
    "\n",
    "  if dict_exp_config[\"lr_scheduler\"] is not None:\n",
    "    if dict_exp_config[\"lr_scheduler\"] == \"OneCycleLR\":\n",
    "      scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=dict_exp_config[\"lr\"], steps_per_epoch=dict_exp_config[\"steps_per_epoch\"], epochs=dict_exp_config[\"num_epochs\"], last_epoch=last_epoch_scheduler)\n",
    "    elif dict_exp_config[\"lr_scheduler\"] == \"MultiStepLR\":\n",
    "      scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=dict_exp_config[\"milestones\"], gamma=0.1, last_epoch=last_epoch_scheduler)\n",
    "    elif dict_exp_config[\"lr_scheduler\"] == \"CosineAnnealingLR\":\n",
    "      scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=dict_exp_config[\"steps_per_epoch\"], eta_min=0, last_epoch=last_epoch_scheduler)\n",
    "    elif dict_exp_config[\"lr_scheduler\"] == \"CosineWarmupScheduler\":\n",
    "      scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=dict_exp_config[\"warmup\"], max_iters=dict_exp_config[\"num_epochs\"], last_epoch=last_epoch_scheduler)\n",
    "  else:\n",
    "    scheduler = None\n",
    "\n",
    "  if scheduler is not None:\n",
    "    scheduler.load_state_dict(scheduler_state_dict)\n",
    "\n",
    "  if resume_training:\n",
    "    pyro.get_param_store().load(path_last_model_params)\n",
    "  else:\n",
    "    pyro.get_param_store().load(path_best_model_params)\n",
    "\n",
    "  model.to(device)\n",
    "\n",
    "  if eval_model == False:\n",
    "    bar = trange(epoch, dict_exp_config[\"num_epochs\"])\n",
    "else:\n",
    "  bar = trange(dict_exp_config[\"num_epochs\"])\n",
    "\n",
    "if eval_model == False:\n",
    "  \n",
    "  model.train()\n",
    "  for epoch in bar:\n",
    "\n",
    "    file_log.write(\"Epoch: {}\\n\".format(epoch))\n",
    "    file_log.flush()\n",
    "\n",
    "    total_epoch_loss_train = train(loss, train_loader, optimizer, scheduler, use_cuda=USE_CUDA)\n",
    "    tot_train_elbo.append(total_epoch_loss_train)\n",
    "    file_loss_train.write(str(total_epoch_loss_train) + \"\\n\")\n",
    "    file_loss_train.flush()\n",
    "\n",
    "    if epoch == 0:\n",
    "      tot_number_of_paramaters = total_number_of_params(loss.parameters())\n",
    "      print(\"Number of model parameters: {}\".format(tot_number_of_paramaters))\n",
    "      with open(path_architecture_txt, 'a') as f:\n",
    "        f.write(\"Number of model parameters: {}\".format(tot_number_of_paramaters))\n",
    "\n",
    "    bar.set_postfix(tot_loss=f'{total_epoch_loss_train:.3f}')\n",
    "    \n",
    "    if epoch % dict_exp_config[\"eval_epoch\"] == 0 and epoch > 0:\n",
    "      \n",
    "      print(\"\\n------------- Epoch {} Validation -------------\".format(epoch))\n",
    "      file_log.write(\"\\n------------- Epoch {} Validation -------------\\n\".format(epoch))\n",
    "      file_log.flush()\n",
    "\n",
    "      tot_mse_loss_test = 0.0\n",
    "      tot_mse_loss_test_org = 0.0\n",
    "      total_epoch_loss_test = 0.0\n",
    "\n",
    "      for key in info_cod_stations.keys():\n",
    "        \n",
    "        cod_station = dict_datasets_test[key].cod_station\n",
    "        x_test_tensor = dict_datasets_test[key].x_tensor\n",
    "        y_test_tensor = dict_datasets_test[key].y_tensor\n",
    "        mask = dict_datasets_test[key].mask\n",
    "        mask = torch.from_numpy(mask).to(device)\n",
    "        dates = dict_datasets_test[key].get_list_info_date()\n",
    "\n",
    "        min_cod_station = info_cod_stations[key][5]\n",
    "        max_cod_station = info_cod_stations[key][6]\n",
    "\n",
    "        mean_cod_station = dict_datasets_test[key].mean\n",
    "        std_cod_station = dict_datasets_test[key].std\n",
    "\n",
    "        epoch_loss_test = evaluate(loss, x_test_tensor, y_test_tensor, dates, mask, use_cuda=USE_CUDA)\n",
    "        total_epoch_loss_test += epoch_loss_test\n",
    "\n",
    "        predictive = Predictive(model, guide=guide, num_samples=dict_exp_config[\"num_samples_pred\"])\n",
    "        preds_test = predictive(x=x_test_tensor.to(device), date=dates, mask=mask, y=None)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            y_pred_test = preds_test['obs'].T.cpu().detach().numpy().mean(axis=1)\n",
    "        else:\n",
    "            y_pred_test = preds_test['obs'].T.detach().numpy().mean(axis=1)\n",
    "\n",
    "        if standardization_data:\n",
    "            if std_cod_station > 0.0:\n",
    "              y_test_tensor = (y_test_tensor * std_cod_station) + mean_cod_station\n",
    "              y_pred_test = (y_pred_test * std_cod_station) + mean_cod_station\n",
    "        else:\n",
    "            y_test_tensor = y_test_tensor + mean_cod_station\n",
    "            x_test_tensor = y_pred_test + mean_cod_station\n",
    "\n",
    "        mse_loss_test = mean_squared_error(y_test_tensor, y_pred_test)\n",
    "\n",
    "        if org_data == False:\n",
    "          y_test_org = inverse_transform_minmax(y_test_tensor, min_cod_station, max_cod_station)\n",
    "          y_test_pred_org = inverse_transform_minmax(y_pred_test, min_cod_station, max_cod_station)\n",
    "          mse_loss_test_org = mean_squared_error(y_test_org, y_test_pred_org)\n",
    "\n",
    "          print(\"Current cod station: {} - MSE test: {} - MSE test org: {}\".format(cod_station, mse_loss_test, mse_loss_test_org))\n",
    "          file_log.write(\"Current cod station: {} - MSE test: {} - MSE test org: {}\\n\".format(cod_station, mse_loss_test, mse_loss_test_org))\n",
    "          file_log.flush()\n",
    "\n",
    "          tot_mse_loss_test_org += mse_loss_test_org\n",
    "        else:\n",
    "          print(\"Current cod station: {} - MSE test: {}\".format(cod_station, mse_loss_test))\n",
    "          file_log.write(\"Current cod station: {} - MSE test: {}\\n\".format(cod_station, mse_loss_test))\n",
    "          file_log.flush()\n",
    "\n",
    "        tot_mse_loss_test += mse_loss_test\n",
    "        \n",
    "      tot_mse_loss_test /= len(info_cod_stations)\n",
    "      total_epoch_loss_test /= len(info_cod_stations)\n",
    "\n",
    "      if org_data == False:\n",
    "        tot_mse_loss_test_org /= len(info_cod_stations)\n",
    "\n",
    "      test_elbo.append(total_epoch_loss_test)\n",
    "      file_loss_test.write(str(total_epoch_loss_test) + \"\\n\")\n",
    "      file_loss_test.flush()\n",
    "\n",
    "      if best_mse_test > tot_mse_loss_test:\n",
    "        best_epoch = epoch\n",
    "        best_mse_test = tot_mse_loss_test\n",
    "\n",
    "        if org_data == False:\n",
    "          best_mse_test_org = tot_mse_loss_test_org\n",
    "\n",
    "        best_elbo_test = total_epoch_loss_test\n",
    "        best_model_state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "        print(\"\\nBest MSE test: {} - epoch: {}\".format(best_mse_test, best_epoch))\n",
    "        if org_data == False:\n",
    "          print(\"Best MSE test org: {} - epoch: {}\".format(best_mse_test_org, best_epoch))\n",
    "        print(\"ELBO loss test: {} - epoch: {}\".format(best_elbo_test, best_epoch))\n",
    "\n",
    "        file_log.write(\"\\nBest MSE test: {} - epoch: {}\\n\".format(best_mse_test, best_epoch))\n",
    "        if org_data == False:\n",
    "          file_log.write(\"Best MSE test org: {} - epoch: {}\\n\".format(best_mse_test_org, best_epoch))\n",
    "        file_log.write(\"ELBO loss test: {} - epoch: {}\\n\".format(best_elbo_test, best_epoch))\n",
    "        file_log.flush()\n",
    "\n",
    "        if os.path.exists(path_best_model_txt):\n",
    "          os.remove(path_best_model_txt)    \n",
    "\n",
    "        file_best_model = open(path_best_model_txt, \"w\")\n",
    "        file_best_model.write(str(best_epoch) + \"\\n\")\n",
    "        file_best_model.write(str(best_mse_test) + \"\\n\")\n",
    "        if org_data == False:\n",
    "          file_best_model.write(str(best_mse_test_org) + \"\\n\")\n",
    "        file_best_model.write(str(best_elbo_test) + \"\\n\")\n",
    "        file_best_model.flush()\n",
    "        file_best_model.close()\n",
    "\n",
    "        # Save best model param\n",
    "        save_checkpoint(best_model_state_dict, guide, optimizer, scheduler, epoch, path_best_model, path_best_model_params)\n",
    "\n",
    "      else:\n",
    "        print(\"\\nCurrent MSE test: {} - Best MSE: {} - epoch: {}\".format(tot_mse_loss_test, best_mse_test, best_epoch))\n",
    "        if org_data == False:\n",
    "          print(\"Current MSE test org: {} - Best MSE org: {} - epoch: {}\".format(tot_mse_loss_test_org, best_mse_test_org, best_epoch))\n",
    "        print(\"Current loss test: {} - Best loss test: {} - epoch: {}\".format(total_epoch_loss_test, best_elbo_test, best_epoch))\n",
    "        \n",
    "        file_log.write(\"\\nCurrent MSE test: {} - Best MSE: {} - epoch: {}\\n\".format(tot_mse_loss_test, best_mse_test, best_epoch))\n",
    "        if org_data == False:\n",
    "          file_log.write(\"Current MSE test org: {} - Best MSE org: {} - epoch: {}\\n\".format(tot_mse_loss_test_org, best_mse_test_org, best_epoch))\n",
    "        file_log.write(\"Current loss test: {} - Best loss test: {} - epoch: {}\\n\".format(total_epoch_loss_test, best_elbo_test, best_epoch))\n",
    "        file_log.flush()\n",
    "\n",
    "    # Save last model param\n",
    "    model_state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "    save_checkpoint(model_state_dict, guide, optimizer, scheduler, epoch, path_last_model, path_last_model_params)\n",
    "\n",
    "# --------------------- Prediction phase ---------------------\n",
    "pyro.clear_param_store()\n",
    "del model\n",
    "del guide\n",
    "\n",
    "print(\"\\n------------- Prediction phase -------------\")\n",
    "file_log.write(\"\\n------------- Prediction phase -------------\\n\")\n",
    "file_log.flush()\n",
    "\n",
    "# Create a model and a guide, both as (Pyro)Modules.\n",
    "model: torch.nn.Module = Model( len_dataset=len(train_loader.dataset), input_features=dict_model_config[\"input_features\"], seq_len=max_model_order, \n",
    "                                embedding_dim=dict_model_config[\"embedding_dim\"], num_enc_block=dict_model_config[\"num_enc_block\"], num_heads=dict_model_config[\"num_heads\"], \n",
    "                                h_enc_layer=dict_model_config[\"h_enc_layer\"],  dropout=dict_model_config[\"dropout\"], h1=dict_model_config[\"h1\"], h2=dict_model_config[\"h2\"], \n",
    "                                add_positional_encoding=dict_model_config[\"add_positional_encoding\"],\n",
    "                                add_temporal_embedding=dict_model_config[\"add_temporal_embedding\"],\n",
    "                                sigma_layer=dict_model_config[\"sigma_layer\"], device=device).to(device)\n",
    "\n",
    "model_state_dict, guide, _, _, epoch = load_checkpoint(path_best_model)\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "pyro.get_param_store().load(path_best_model_params)\n",
    "model.to(device)\n",
    "\n",
    "print(\"Best epoch: {}\\n\".format(best_epoch-1))\n",
    "file_log.write(\"Best epoch: {}\\n\\n\".format(best_epoch-1))\n",
    "file_log.flush()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Predictive distribution\n",
    "# num_samples_pred: Number of realizations for each timestamp t\n",
    "predictive = Predictive(model, guide=guide, num_samples=dict_exp_config[\"num_samples_pred\"])\n",
    "\n",
    "for key in info_cod_stations.keys():\n",
    "  \n",
    "  train_dataset = dict_datasets_train[key]\n",
    "\n",
    "  current_cod_station = dict_datasets_test[key].cod_station\n",
    "  x_test = dict_datasets_test[key].x_tensor\n",
    "  y_test = dict_datasets_test[key].y_tensor\n",
    "  dates_test = dict_datasets_test[key].get_list_info_date()\n",
    "  \n",
    "  region = info_cod_stations[key][2]\n",
    "  start_year = info_cod_stations[key][3]\n",
    "  end_year = info_cod_stations[key][4]\n",
    "\n",
    "  min_cod_station = info_cod_stations[key][5]\n",
    "  max_cod_station = info_cod_stations[key][6]\n",
    "\n",
    "  start_date_current_cod_station = datetime(start_year, 1, 1, 0, 0)\n",
    "  end_date_current_cod_station = datetime(end_year+1, 1, 1, 0, 0)\n",
    "\n",
    "  mean_cod_station = train_dataset.mean\n",
    "  std_cod_station = train_dataset.std\n",
    "  \n",
    "  path_dir_plots_current_station = joinpath(path_dir_plots, current_cod_station)\n",
    "  os.makedirs(path_dir_plots_current_station, exist_ok=True)\n",
    "\n",
    "  x_train_to_use = train_dataset.x_train_to_use\n",
    "  y_train_to_use = train_dataset.y_train_to_use\n",
    "  x_train = train_dataset.x_train\n",
    "  y_train = train_dataset.y_train\n",
    "  x_train_tensor = train_dataset.x_train_tensor\n",
    "\n",
    "  model_order = train_dataset.model_order\n",
    "\n",
    "  idx_complete_ts = all_idx_missing_values_complete[key]\n",
    "\n",
    "  mask = train_dataset.mask\n",
    "  mask = torch.from_numpy(mask).to(device)\n",
    "\n",
    "  dates_train = train_dataset.get_list_info_date()\n",
    "  idx_missing_values = train_dataset.idx_missing_values\n",
    "\n",
    "  path_dir_plots_current_station_calibr_plot = joinpath(path_dir_plots_current_station, \"Calibration Plot\")\n",
    "  os.makedirs(path_dir_plots_current_station_calibr_plot, exist_ok=True)\n",
    "  \n",
    "  y_pred_mean_train, y_pred_median_train, ci_train, \\\n",
    "  y_pred_mean_test, y_pred_median_test, ci_test, \\\n",
    "  x_train, x_test = \\\n",
    "    prediction_phase( current_cod_station, region, start_year, end_year, predictive, \n",
    "                      x_train, y_train, y_test, x_test,\n",
    "                      min_cod_station, max_cod_station, \n",
    "                      mean_cod_station, std_cod_station,\n",
    "                      mask, dates_train, dates_test,\n",
    "                      path_csv_errors_cod_stations, device, USE_CUDA,\n",
    "                      org_data, standardization_data,\n",
    "                      dict_exp_config[\"num_samples_pred\"],\n",
    "                      dict_exp_config[\"delta_conf_inter\"],\n",
    "                      dict_exp_config[\"confidence_level\"],\n",
    "                      path_dir_plots_current_station_calibr_plot,\n",
    "                      path_csv_calibration_errors_cod_stations_train,\n",
    "                      path_csv_calibration_errors_cod_stations_test,\n",
    "                      path_csv_ence_errors_cod_stations_train,\n",
    "                      path_csv_ence_errors_cod_stations_test,\n",
    "                      model_name\n",
    "              )\n",
    "  \n",
    "  if dict_exp_config[\"type_prediction\"] == \"mean\":\n",
    "    y_pred_train = y_pred_mean_train\n",
    "    y_pred_test = y_pred_mean_test\n",
    "\n",
    "  # --------------------- Imputation phase ---------------------\n",
    "  imputation(\n",
    "                x_train, y_train, y_pred_train, idx_missing_values,\n",
    "                model_order, max_model_order, key, all_new_x_train, all_new_y_train,\n",
    "            )\n",
    "    \n",
    "  # --------------------- Reconstruction complete time series ---------------------\n",
    "  N = all_idx_train[key].shape[0] + all_idx_test[key].shape[0] + model_order\n",
    "\n",
    "  old_complete_ts_current_cod_station, new_complete_ts_current_cod_station, new_complete_ts_current_cod_station_ci = \\\n",
    "    reconstruct_complete_ts(  x_train, y_train, ci_train, x_test, y_test, ci_test,\n",
    "                              all_new_x_train[key], all_new_y_train[key], all_idx_train[key], all_idx_test[key], \n",
    "                              min_cod_station, max_cod_station, N, model_order, max_model_order,\n",
    "                              org_data,\n",
    "                          )\n",
    "  \n",
    "  # --------------------- Plot complete time series ---------------------\n",
    "  path_dir_plots_current_station_ts = joinpath(path_dir_plots_current_station, \"Time Series\")\n",
    "  os.makedirs(path_dir_plots_current_station_ts, exist_ok=True)\n",
    "  \n",
    "  plot_ts(\n",
    "              current_cod_station, \n",
    "              old_complete_ts_current_cod_station, \n",
    "              new_complete_ts_current_cod_station, \n",
    "              new_complete_ts_current_cod_station_ci,\n",
    "              idx_complete_ts, path_dir_plots_current_station_ts,\n",
    "              start_date, end_date,\n",
    "              start_date_current_cod_station, end_date_current_cod_station, \n",
    "              dict_limit_air_pollutants, air_poll_selected, delta_time, co_in_ug_m3,\n",
    "              dict_exp_config[\"confidence_level\"],\n",
    "              model_name\n",
    "            )\n",
    "\n",
    "  # Plot training set time series segments\n",
    "  plot_ts_segments(\n",
    "                    current_cod_station, \n",
    "                    x_train, \n",
    "                    y_train,\n",
    "                    y_pred_train, \n",
    "                    ci_train, \n",
    "                    idx_complete_ts,\n",
    "                    start_date, \n",
    "                    min_cod_station, \n",
    "                    max_cod_station, \n",
    "                    model_order, \n",
    "                    air_poll_selected, \n",
    "                    co_in_ug_m3, path_dir_plots_current_station_ts, \n",
    "                    dict_exp_config[\"n_segments\"],\n",
    "                    dict_exp_config[\"confidence_level\"],\n",
    "                    min_cod_station, max_cod_station, \n",
    "                    mean_cod_station, std_cod_station,\n",
    "                    standardization_data,\n",
    "                    org_data,\n",
    "                    train = True,\n",
    "                    model_name = model_name\n",
    "            )\n",
    "  \n",
    "  # Plot test set time series segments\n",
    "  plot_ts_segments(\n",
    "                    current_cod_station, \n",
    "                    x_test, \n",
    "                    y_test,\n",
    "                    y_pred_test, \n",
    "                    ci_test, \n",
    "                    idx_complete_ts,\n",
    "                    start_date, \n",
    "                    min_cod_station, \n",
    "                    max_cod_station, \n",
    "                    model_order, \n",
    "                    air_poll_selected, \n",
    "                    co_in_ug_m3, path_dir_plots_current_station_ts, \n",
    "                    dict_exp_config[\"n_segments\"],\n",
    "                    dict_exp_config[\"confidence_level\"],\n",
    "                    min_cod_station, max_cod_station, \n",
    "                    mean_cod_station, std_cod_station,\n",
    "                    standardization_data,\n",
    "                    org_data,\n",
    "                    train = False,\n",
    "                    model_name = model_name\n",
    "            )\n",
    "\n",
    "  if eval_model == False:\n",
    "    with open(path_cod_stations_processed, 'a') as f:\n",
    "      f.write(current_cod_station + \"\\n\")\n",
    "\n",
    "  '''\n",
    "  else:\n",
    "  \n",
    "    save_csv_ts(current_cod_station, new_complete_ts_current_cod_station,\n",
    "                new_complete_ts_current_cod_station_std, N, path_csv_ts_cod_stations,\n",
    "                start_date, delta_time, air_poll_selected)\n",
    "  '''\n",
    "\n",
    "path_dir_plots_losses = joinpath(path_dir_plots, \"Losses\")\n",
    "os.makedirs(path_dir_plots_losses, exist_ok=True)\n",
    "\n",
    "path_tot_loss_training = joinpath(path_dir_plots_losses, \"tot_loss_train.png\")\n",
    "plot_loss(tot_train_elbo, \"Tot loss training set\", path_tot_loss_training)\n",
    "\n",
    "path_tot_loss_test = joinpath(path_dir_plots_losses, \"tot_loss_test.png\")\n",
    "plot_loss(test_elbo, \"Tot loss test set\", path_tot_loss_test)\n",
    "\n",
    "if eval_model:\n",
    "  # Save dictionaries of all cod stations selected\n",
    "  path_dir_results_datasets = joinpath(path_dir_model_config, \"pickles\")\n",
    "  os.makedirs(path_dir_results_datasets, exist_ok=True)\n",
    "\n",
    "  save_pickle_datasets( \n",
    "                        all_idx_train, all_idx_test, all_idx_missing_values_train,\n",
    "                        all_idx_missing_values_complete, all_new_x_train, all_new_y_train, \n",
    "                        all_x_test, all_y_test,\n",
    "                        info_cod_stations, path_dir_results_datasets\n",
    "                      )\n",
    "    \n",
    "pyro.clear_param_store()\n",
    "del model\n",
    "del guide\n",
    "\n",
    "clear_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
